\section{Results}
\label{results}

We start by describing the results for each study in detail, before proceeding to summarize and discuss the observed effects.

\subsection{Experimental Simulation}
\label{sec:expResults}

%demographics
Participants mixed graduates working in industry (41.6\%) and graduates during their M.Sc. and Ph.D. studies (58.4\%).
%table
In Table~\ref{tab:results} we summarize the results from the 24 trials, from which five -- 21\% -- were delivered with at least one fault detected by our test cases.
%individual results
All participants assigned to textual specifications in Javadoc delivered correct programs.
By contrast, half of the programs delivered by participants using APIs with formal contracts were faulty (four out of eight).
One faulty program was delivered by a participant from the \contractjdoc{} group.
%task
Regarding task, faulty programs were mostly present in implementing in API implementations -- four of them, if compared with only one in API clients.


% source code correct
%Javadoc and \contractjdoc{} were the only documenting approaches in which all participants were able to produce a code satisfying the oracle (respecting the restrictions available in the comments). On the other hand, there was one case developed by following the JML documenting approach in which the contract is not satisfied by the implementation.



\begin{table}
\centering
\caption{Experimental results, for each treatment (textual Javadoc \emph{JavaDoc}, \contractjdoc{} \emph{ContJDoc} and formal contracts \emph{Formal}. For each API (Queue or Stack) and Task (\emph{Cli} if a client for the API was implemented, \emph{Sup} if an implementation for the API was provided), participants are listed (\emph{Part}) along with the result (\emph{Res}) from our test cases.}
\label{tab:results}
\begin{tabular}{|l|l||l|l||l|l||l|l|} 
\cline{3-8}
\multicolumn{1}{l}{} &              & \multicolumn{2}{l||}{\textbf{JavaDoc }} & \multicolumn{2}{l||}{\textbf{ContJDoc }} & \multicolumn{2}{l|}{\textbf{Formal}}  \\ 
\hline
\uline{DataStr}      & \uline{Task} & Part & Res                              & Part & Res                                & Part & Res                             \\ 
\hline\hline
\uline{Queue}        & \uline{Cli}  & p9   & \greencheck       & p11  & \greencheck                                    & p7   & \greencheck                                 \\
                     & \uline{Cli}  & p10  & \greencheck                                 & p12  & \greencheck                                    & p8   & \greencheck                                \\
                     & \uline{Sup}  & p21  & \greencheck                                 & p23  & \greencheck                                    & p19  &  \greencheck                               \\
                     & \uline{Sup}  & p22  & \greencheck                                 & p24  & \greencheck                                    & p20  &  \redcross                               \\ 
\hline\hline
\uline{Stack}        & \uline{Cli}  & p3   & \greencheck                                 & p5   & \greencheck                                    & p1   & \redcross                                 \\
                     & \uline{Cli}  & p4   & \greencheck                                 & p6   & \greencheck                                    & p2   & \greencheck                                 \\
                     & \uline{Sup}  & p15  & \greencheck                                 & p17  & \redcross                                    & p13  & \redcross                                \\
                     & \uline{Sup}  & p16  & \greencheck                                 & p18  & \greencheck                                    & p14  & \redcross                                \\
\hline
\end{tabular}
\end{table}

%faults
Table~\ref{tab:faults} details the faults detected by the test cases. After detailed analysis of the programs, we classified each fault in terms of the violated contract. 
%clients
All clients fulfilled the specified pre-conditions for calling the API methods -- p1's API client raise an exception which was incompatible with the method's postcondition.
%suppliers
All four faults unveiled in API implementations resulted from failure in satisfying the specified post-condition in methods removing data from the given structure (two on \texttt{Stack.pop} and two on \texttt{Queue.remove}).

\begin{table}
\centering
\caption{Reason (fault) for failures in participants' results}
\label{tab:faults}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{|l|l|l|l|} 
\hline
\multicolumn{4}{|l|}{\textbf{ContractJDoc }}                                                                                                                                                                    \\ 
\hline
p17                                        & Stack                             & Sup                                & Post-condition violation on method \texttt{remove}                                    \\ 
\hline
\multicolumn{4}{|l|}{\textbf{Formal contracts} }                                                                                                                                                                \\ 
\hline
p1                                         & Stack                             & Cli                                & Post-condition violation on method \texttt{removeAccountTop})    \\ 
\hline
p13                                        & Stack                             & Sup                                & Post-condition violation on method \texttt{pop})     \\ 
\hline
p14                                        & Stack                             & Sup                                & Post-condition violation on method \texttt{pop})     \\ 
\hline
p20                                        & Queue                             & Sup                                & Post-condition violation on method \texttt{remove})  \\ 
\hline\hline
\multicolumn{1}{|c|}{\textbf{Part}} & \multicolumn{1}{c|}{\textbf{API}} & \multicolumn{1}{c|}{\textbf{Task}} & \multicolumn{1}{c|}{\textbf{Type of~Fault}}                                               \\
\hline
\end{tabular}
\end{adjustbox}
\end{table}

After they sent their code back, we asked participants about understandability of the API specifications, using a Likert-like scale which ranges from 1 (less understandable) to 5 (very understandable). The results for 24 answers are summarized in Figure~\ref{fig:ExpAnswersTotal}, where we can see assessments are split between 3 to 5. Most participants evaluated understandability as 4 (58\%).

\begin{figure*}
\centering
\begin{subfigure}{.32\textwidth}
\includegraphics[width=1\textwidth]{figs/ExpAnswersTotal.png}
\caption{Total Results.}
\label{fig:ExpAnswersTotal}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
\includegraphics[width=1\linewidth]{figs/boxplotApproachesEmpiricalStudy}
\caption{Results by approach.}
\label{fig:approachesEmpirical}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
\includegraphics[width=1\linewidth]{figs/boxplotTasksEmpiricalStudy}
\caption{Results by task.}
\label{fig:tasksEmpirical}
\end{subfigure}
\caption{Results from Understandability Assessment of API specifications, as Perceived from Participants.}
\label{fig:empiricalResults}
\end{figure*}

%perspectives
Assuming a distinct perspective, we grouped understandability assessments by the assigned documentation approach and task -- Figures~\ref{fig:approachesEmpirical} and~\ref{fig:tasksEmpirical}, respectively, depict their distribution using boxplots.
%doc approach
Visually, the assessments for Javadoc APIs are all above the median ($4$), while \contractjdoc{} assessments fluctuate around that median value. Formal contracts, in turn, were all assessed as $3$ or $4$.  
% stats
Nevertheless, statistical tests (in this case, Kruskal-Wallis non-parametric test) showed no difference between the groups (p-value = $0.15$, 95\% confidence level).

%task
Concerning the task performed by the developers, higher understandability is reported by participants assigned to supplier tasks, at least visually. Still, the Wilcoxon rank sum test~\cite{statistical} reported no significant difference (p-value = 0.07, for confidence level of 95\%).

%qualitative results
Furthermore, we asked the participants, with an open-ended question, to provide comments on their tasks. 
We used \emph{open coding}~\cite{openCoding} to analyze the collected answers, by inspecting them quote by quote and detecting categories, representing the key ideas in the data.
The quotes were organized in 11 categories, which are presented in Table~\ref{tab:categories}, in conjunction with the number of quotes. One quote might classified in more than one category -- three participants did not provide any answer.

%on the results
%seven
We found that most quotes either value the specifications or suggest the contracts should have been stronger (seven each). As an example of the latter, p6, which was assigned an \contractjdoc{} API, wrote "I hesitated over the \texttt{pop} method, due to its exception; there should be more information about the exception to be thrown in each case". 
%four
Others made explicit suggestions on how the specification should be, from their previous understanding of Queue and Stack APIs.
%other
Interestingly, we found a few quotes that expressed total trust in the contracts, dismissing defensive programming as advocated by the Design by Contract methodology ~\cite{dbc}.
Another category encompasses quotes that remark how the Javadoc text around the contract expressions helped them to understand the API specification. 

\begin{table}
\centering
\caption{Categories in Participants' quotes}
\label{tab:categories}
\begin{tabular}{|l|r|} 
\hline
\textbf{Category description}                & \multicolumn{1}{l|}{\textbf{\#quotes}}  \\ 
\hline\hline
Specifications were clear and understandable & 7                                       \\ 
\hline
Specifications were vague                    & 7                                       \\ 
\hline
Suggestions to change the specification      & 4                                       \\ 
\hline
The surrounding text helped understanding    & 2                                       \\ 
\hline
Total trust in the contracts                 & 2                                       \\ 
\hline
Contracts were ignored                       & 1                                       \\ 
\hline
Hard to read the logic                       & 1                                       \\ 
\hline
Questions about the task                     & 1                                       \\ 
\hline
Text was confusing                           & 1                                       \\
\hline
\end{tabular}
\end{table}


\subsection{Judgment Survey}
\label{sec:surveyResults}

%survey description
As a follow-up for the lab study, we showed three versions of the Stack API as a web survey, asking questions about those approaches to contract specifications; we suggested respondents to play the client role of the API, as if they were about to instantiate it and call its methods.
%questions
Respondents were asked to assess understandability of each approach (again, using a 1--5 scale), in addition to providing a verdict asking which approach they think is the most understandable as an API specification. For this, they were given three options -- one for each approach -- and a fourth, neutral option.

%general results
From the universe of developers who received the invitation, answers from 142 Java developers were considered valid.
%demographics
%students and professionals
From those, 51 are graduate professionals (36\%) and 91 are undergraduate and graduate students in computer-related degrees (64\%).
%experience
Having some experience with Java was a requirement for being a participant; more than 73\% (105) have experience with either open source or industrial projects. Likewise, 74\% (105) of the respondents declared to have more than one year of Java programming.
%contract experience.
Regarding formal contract languages for specifying APIs (Eiffel, JML or similar), most respondents -- 67\% -- had no previous experience. 


%results - javadoc is the simplest
With respect to the survey answers, it is showed in Figure~\ref{fig:mostUnderstandable} that 50.7\% (72) of the respondents chose Javadoc as the simplest approach to understand the provided API specification. 
%contractjdoc em JML
While only 11 (7\%) chose formal contracts as the most understandable approach and 18 (12\%) were indifferent, a almost one third of the respondents (41, almost 29\%) chose the mix of text and contract expressions -- \contractjdoc{}.


\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{figs/mostUnderstandable.png}
\label{fig:mostUnderstandable}
\caption{Surveyed Verdicts on the Most Understandable API Documentation Approach.}
\label{fig:surveyResults}
\end{figure}





% TO FINISH
The survey results provided us statistical difference when comparing the the
comprehensibility of the documentation approaches evaluated (see
Figure~\ref{fig:allApproaches}). By performing an Oneway ANOVA test~\cite{statistical} and
a corresponding post hoc analysis we were able to distinguish the three
approaches (p-value $<$ 0.05).
The Tukey HSD~\cite{statistical} and pairwise comparisons using t tests
with Bonferroni correction~\cite{statistical} produced the following p-values:
Javadoc-ContractJDoc = 0.012, JML-ContractJDoc = 0.000, and JML-Javadoc = 0.000.


When analyzing data grouped by experience (Figures~\ref{fig:javadocExp} to
~\ref{fig:jmlExp}) by means of Wilcoxon rank sum test with continuity correction
tests, only for JML we found no statistical difference between Professionals and
Students (p-value = 0.17). For both Javadoc and \contractjdoc{}, Professionals
had perceived the approaches as being easier for comprehensing than Students
(p-value = 0.012 and p-value = 0.004, respectively).






\begin{figure*}
\centering
\begin{subfigure}{.48\textwidth}
\includegraphics[width=1\linewidth]{figs/boxplotApproachesSurveyStudy}
\caption{All approaches}
\label{fig:allApproaches}
\end{subfigure}

\caption{Subjects' answers to the individual evaluation of comprehensibility for
each documentation approach. And answers grouped by experience for each
approach.}
\label{fig:surveyResults}
\end{figure*}



\subsection{Case Study}

Table~\ref{tab:caseStudyResults} presents the results of applying \contractjdoc{} to each system.
Column \#Clauses displays the number of clauses manually added in each system.
Column \#Errors presents the number of errors detected by the systems test suite after compiling the source code enhanced with contracts in
\contractjdoc{} approach. Column Time reveals the time (in seconds) needed for compiling the whole
project with its dependencies after applying \contractjdoc{} contracts. Columns \#Com.Case to \#Repet.
show the contract clauses added in each system grouped by type (following the
definitions from ~\cite{typeContracts}).

\begin{table*}[h]
\caption{Case study Results.}
\label{tab:caseStudyResults}
\centering
\begin{tabular}{l l l l l l l l}
\hline
 \bfseries System &
 \bfseries \#Clauses & 
 \bfseries \#Errors & 
 \bfseries Time (s) &
 \bfseries \#Tests &
 \bfseries \#Com.Case &
 \bfseries \#AppSpec. &
 \bfseries \#Repet. \\ \hline
ABC-Music-Player & 115 & 2 & 14 & 30 & 42 & 11 & 62 \\
Dishevelled & 2,655 & 381 & 434 & 2,643 & 1,536 & 151 & 968 \\
Jenerics & 190 & 7 & 20 & 44 & 156 & 0 & 34 \\
OOP Aufgabe3 & 54 & 1 & 4 & 11 & 16 & 30 & 8 \\
SimpleShop & 50 & 0 & 5 & 0 & 30 & 11 & 9 \\
Webprot\'{e}g\'{e} & 930 & 0 & 713 & 0 & 717 & 79 & 133 \\ \hline

 \bfseries Total & 
 \bfseries \totalClauses{} & 
 \bfseries 391 &
 \bfseries 1,185 &
 \bfseries 2,728 &
 \bfseries 2,497 &
 \bfseries 282 &
 \bfseries 1,214
\\
\bottomrule
\end{tabular}
\end{table*}

% Jenerics: 8 - 6 failures, 1 error.

% results for contract types
Concerning the kind of contracts, the only unit in which we wrote more
application-specific contracts was \texttt{OOP Aufgabe3} system (55\% of the
written contracts are application-specific). On the other hand, in \texttt{ABC-Music-Player}, more than 90\% of the contracts remains between common-case and
repetitive code: verifications that strings are not blank, collections are not
empty, or that a method returns a field.
For \texttt{Dishevelled}, the majority of the written contracts is classified as common-case
(57.51\%), other 36.92\% are repetitive with code and only 5.57\% are application-specific.
In addition, all contracts written for \texttt{Jenerics} are related to
verification of nullity from parameters or the return value, thus all contracts
remains between common-case and repetitive code. In \texttt{SimpleShop}, the
written contracts are distributed in the following manner: common-case 60\%, repetitive code 19\%, and application-specific 21\%; again the number of common-case and repetitive code outperforms application-specific contracts. Finally,
in \texttt{WebProt\'{e}g\'{e}}, the distribution is: common-case 77.51\%, repetitive code 14.38\%, and application-specific 8.11\%.  

% conformance errors
When applying \contractjdoc{} to \texttt{ABC-Music-Player}, we found inconsistencies between Javadoc
comments and the source code. The problems occurred in the class \texttt{Utilities} (package
\texttt{sound}) because there are comments concerning a parameter declaring that the value of
this parameter must not be greater than or equal to zero; however in the body of the methods there
is an if-clause that throws exceptions when the value received by the parameter is negative.




