\section{Discussion}
\label{discussion}

This section comprises discussion on the results from our empirical studies: experimental simulation, judgment survey and case study -- using, as basis, the aforementioned research questions, and threats to validity.


\subsection{RQ1. What is the Effect of the API Documentation Approach on API Usage and Implementation Tasks?}
\label{rq1}

%code correctness, each approach
Concerning code correctness, all participants assigned to Javadoc APIs produced code
in accordance with the contracts -- our manually-produced test cases did not detect any contract violation. One \contractjdoc{} API implementation was delivered with a single fault, while half of the participants assigned to API with formal contracts presented at least one fault.
%problems in understanding formal contracts
These results may suggest participants had trouble understanding API formal contracts, assuming their self-reported experience in Java programming and their intention to complete the assignment correctly.
%no test cases
Since we did not provide our test cases, participants were asked to test at their discretion, and failure in fulfilling contracts as specified was not detected by them.
%at least one example
For example, we established the following postcondition for \texttt{Queue.remove}, using a JML-like notation
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,name=figxpi, frame=lines, mathescape=true]
/**
   
\end{lstlisting}
p20's submission presented an implementation to \texttt{remove} \ldots 







% kind of task
Even though the developers have been perceived the supplier task as less
difficult than the client task, they produced code respecting the restrictions
available on the comments more times to the client task. All developers were
able to write a client code in accordance with the restrictions. Maybe the
difficulty reported by the developers is related with the attention required for
using methods provided by an interface: one needs to read the documentation
available in order to know how to use the methods; whereas implementing the
interface is more simple; mainly for the interfaces used in this experiment:
they are traditional and well-known data structures.
The results of this experiment suggest that when writing a client code,
developers tend to pay more attention to the documentation available than when
they are writing a supplier code (implementing a given interface).




\subsection{RQ2. What are the perceived understandability in using three forms of public interface contracts?}
\label{rq2}



% task
\textbf{Q2.} We ask each developer to perform one task:
either implement a given interface or implement a client code for using the
methods provided by an interface -- such the use of an API (Client). 
Although developers have assigned more \emph{Very Easy} and \textit{Easy} for
the supplier task than to the client (see Figure~\ref{fig:empiricalResults}(a)),
the statistical test did not provide evidences for a significant difference
between the difficulty perceived by developers when performing the required
task.

% approach
\textbf{Q3.}  Although not supported by statistical tests since Kruskal-Wallis
rank sum test showed no difference between the approaches (p-value = 0.15), Javadoc and
\contractjdoc{} were perceived as being easier than JML (see Figure~\ref{fig:approachesEmpirical}).
This result indicates \contractjdoc{} as an approach in an intermediate level
between Javadoc and JML, with both providing runtime conformance checking. Therefore, the
proposed approach is promising: \contractjdoc{} is easy to understand
(create a code based on the comments) -- 75\% of the developers answers for
difficulty remains between \textit{Easy} and \textit{Very easy} -- and enables
the runtime checking of the comments by means of \contractjdocCompiler{}
compiler.

According to the statistical tests performed, Javadoc is the most
understandable documentation approach, and \contractjdoc{} is intermediate between JML and
Javadoc, being closer to Javadoc.
This can also be seen in Figure~\ref{fig:allApproaches}.

An interesting result came from the analysis of the difficulty grouped by
experience (Figures~\ref{fig:javadocExp} to ~\ref{fig:jmlExp}): students and
professionals have perceived the same level of difficulty for JML, which is
promissing as contract-based languages are usually considered harder to be
understood by people with less experience (students, in our survey).

Overall, this survey corroborate with the results from our experiment: \contractjdoc{} is
intermediate between Javadoc and JML, being closer to Javadoc with respect to
comprehensibility.
Furthermore, the results highlight Javadoc as the easiest approach concerning the comprehensibility of
the behavior of a documented interface.


\subsection{RQ3. Can inconsistencies in Java systems be uncovered if using runtime-checkable contract expressions?}
\label{rq3}


% applying contractjdoc
For all systems (see Table~\ref{tab:Units}), we
wrote more pre- and postconditions than invariants. This result has two
explanations: first, as expected the amount of Javadoc comments over the
classes' fields in the evaluated systems is low in comparison with the amount
of Javadoc comments over method's parameters and return.

%  -- the only
% exception occurred in \texttt{SimpleShop} system because the developers have used resources from Java
% constraints for declaring some fields as not null, enabling us to establish
% invariants for them; second, even when there are no comments in a method, we are
% able to write pre- and postconditions based on analysis of method's body.

Concerning pre- and postconditions, for \texttt{ABC-Music-Player} and
\texttt{WebProt\'{e}g\'{e}} projects, we wrote almost twice as many postconditions
as preconditions.
In \texttt{ABC-Music-Player} this is related to the number of accessor methods
available and for \texttt{WebProt\'{e}g\'{e}}, the difference is related to
the available comments.
%  and to the contracts we are able to infer from method's
% body.
%
% In addition, based on the comment and the body of a method, establishing a
% postcondition appears to be simpler than a precondition, as we do not know
% the methods' clients beforehand.


We were able to detect potential inconsistencies in \texttt{ABC-Mu\-sic-Player};
the exception will be always thrown, differently from what is expected from the
commentary.
We also found a problem into
\texttt{WebProt\'{e}g\'{e}} project, in the class \texttt{OWLLiteralParser} there was one exception
in the Javadoc tag \texttt{@throws} that was not declared in the throws of the method's signature.

% conformance errors
In addition, sometimes the tests available along with the systems do
not respect the definitions from the Javadoc comments. For instance, when the
comments in natural language from \texttt{ABC-Music-Player} system are turned into
\contractjdoc{} contracts, some tests from
\texttt{MainTest}, \texttt{ParserTest}, and \texttt{SequencePlayerTest} violate
the methods' preconditions from class \texttt{Utilities}, they try to
call \texttt{Utilities}' methods by passing the value zero as the second
parameter, even though the comment declares the second parameter must be greater than zero.
This scenario also occurred in \texttt{Dishevelled} unit, the comments turned
in \contractjdoc{} contracts also enable us to detect some tests
that do not respect the restrictions available in the Javadoc comments.


% conformance errors
When applying \contractjdoc{} to \texttt{ABC-Music-Player}, we found inconsistencies between Javadoc comments and the source code. The problems occurred in the class \texttt{Utilities} (package
\texttt{sound}) because there are comments concerning a parameter declaring that the value of
this parameter must not be greater than or equal to zero; however in the body of the methods there
is an if-clause that throws exceptions when the value received by the parameter is negative.

As a proof of concept, \contractjdoc{} and its compiler (\contractjdocCompiler{}) enabled us to write runtime
checkable code for third-party systems based on the comments in natural
language.
As expected, the quality and variety of the contracts depended strongly on the available comments, however, we were able to
detect and correct inconsistencies and missing expressions between source code and comments.




\subsection{Limitations and Threats to validity}
\label{sec:CaseStudyThreats}

External validity refers to generalization. Due to its size, results from the
case study cannot be generalized; its purpose is evaluating applicability and relative usefulness.
%represetantive sample
The sample is not representative, since there is no available estimate of the Javadoc-rich project
population in GitHub, then probability sample is impossible.
Our approach is as systematic as feasible in selecting the evaluated project -- manual translation
does not scale, then the sample contains only \totalSystems{} projects.
Therefore, those systems may not be representative of the real use of Javadoc in real systems; however, we were able
to detect inconsistencies between Javadoc comments and source code, as occurred in
\texttt{Utilities} class (\texttt{ABC-Music-Player} experimental unit) in which the comment for a
parameter of the methods is the right opposite of the expected behavior in the source code.

Another risk is that only 24 developers participated in the experimental study
and 142 in the survey and those samples are not representative for the community
of Java developers. Furthermore, we used only two similar data structure interfaces (queue and stack). In other domains with more complex structures, the results
may vary. In addition, the survey used only one data structure interface:
\texttt{Stack}, for asking about the comprehensibility of the interface behavior.
In other domains with more complex structures, the results can vary considerably.


Internal validity refers to causation: are changes in the dependent variable
necessarily the result of manipulations to treatments? All material for the
empirical study and the survey study is available only in English, therefore,
the experience of the Subject with English can have affected their
comprehensibility of the behavior of the provided interface.

%webprotege much bigger
Construct validity refers to correctly measuring the
dependent variable. \texttt{Dishevelled} and
\texttt{WebProt\'{e}g\'{e}} sizes set them apart from the other systems.
For instance, \texttt{Dishevelled} is more than 56 times bigger than \texttt{ABC-Music-Player}, 43
times bigger than \texttt{Jenerics}, 313 times bigger than \texttt{OOP Aufgabe3}, and 234 times
bigger than \texttt{SimpleShop}.
In order to reduce the threat on the manually-defined contracts,
all systems were annotated and reviewed by three researchers, separately.

The order in which we display the documented
interfaces on the survey form, the questions used for evaluating
comprehensibility, the kind of questions used, and the absence of opened questions
can also threat the construct validity. For dealing with these threats we
perform a pilot before applying the survey and used the results from the pilot
to improve the survey structure. In addition, the answers from developers may not be
representative of their real opinion on difficulty perception; to overcome this
threat we made a space for comments available along with the Likert-scale
questions, which are taken into account when collecting the answers.
